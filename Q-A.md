Q1:为什么RNN 需要mask 输入，而CNN则不需要？
对于变长的句子或者文本输入，为什么RNN，LSTM 处理的时候，需要mask 输入屏蔽掉一部分文本，而CNN不需要呢？
A:对于RNN来说，如果不用mask而是补0的话，补0的位置也会参与状态向量的计算，用mask和补0相比，得到的状态向量是不一样的。
因为RNN状态向量计算的时候不仅仅考虑了当前输入，也考虑了上一次的状态向量，因此靠补0的方式进行屏蔽是不彻底的。而CNN是卷积操作，
补0的位置对卷积结果没有影响，即补0和mask两种方式的结果是一样的，因此大家为了省事起见，就普遍在CNN使用补0的方法了。

